{
  "cells":[
    {
      "cell_type":"code",
      "source":[
        "''' install required libraries '''\n",
        "\n",
        "!pip3 install textblob\n",
        "!pip3 install nltk\n",
        "!pip3 install wordcloud\n",
        "!pip3 install tweepy\n",
        "!pip3 install langdetect\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "import pandas as pd\n",
        "import re ,string, csv\n",
        "\n",
        "import tweepy # to access tweet API\n",
        "from tweepy import OAuthHandler # for Authentication\n",
        "\n",
        "from textblob import TextBlob #for Valance of Sentence(polarity)\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "# nltk.download('all') # Installing All from NLTK library\n",
        "from nltk.corpus import stopwords # For Removing Stop words like < the , an , is ,..etc >\n",
        "n_words= stopwords.words('english') #specify english stop words only\n",
        "n_words.append(\"rt\") #append rt for stop word dictionary\n",
        "\n",
        "from nltk.tokenize import word_tokenize # for Tokenizing the sentnces as tokens\n",
        "from nltk.stem.porter import PorterStemmer # converting words to their root forms ,speed and simplicity\n",
        "porter = PorterStemmer() #Create stemmer obejct\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer # also converting words to their actual root forms(noun , verb ,aobjective) ,but it slow\n",
        "lemmatizer = WordNetLemmatizer() #Create lemmatizer obejct\n",
        "\n",
        "from wordcloud import WordCloud,STOPWORDS #Look at Words with highest Frequency for expression\n",
        "\n",
        "from langdetect import detect_langs # Detect language for each tweets \n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from nltk import ngrams\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn import tree\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count":1,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "Collecting textblob\r\n",
            "  Downloading textblob-0.15.3-py2.py3-none-any.whl (636 kB)\r\n",
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 9.6 MB\/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 10.3 MB\/s eta 0:00:01\r\u001b[K     |█▌                              | 30 kB 7.8 MB\/s eta 0:00:01\r\u001b[K     |██                              | 40 kB 8.7 MB\/s eta 0:00:01\r\u001b[K     |██▋                             | 51 kB 9.2 MB\/s eta 0:00:01\r\u001b[K     |███                             | 61 kB 9.1 MB\/s eta 0:00:01\r\u001b[K     |███▋                            | 71 kB 8.3 MB\/s eta 0:00:01\r\u001b[K     |████▏                           | 81 kB 8.6 MB\/s eta 0:00:01\r\u001b[K     |████▋                           | 92 kB 9.4 MB\/s eta 0:00:01\r\u001b[K     |█████▏                          | 102 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |█████▋                          | 112 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |██████▏                         | 122 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |██████▊                         | 133 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |███████▏                        | 143 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |███████▊                        | 153 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |████████▎                       | 163 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |████████▊                       | 174 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |█████████▎                      | 184 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |█████████▉                      | 194 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |██████████▎                     | 204 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |██████████▉                     | 215 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |███████████▎                    | 225 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |███████████▉                    | 235 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |████████████▍                   | 245 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |████████████▉                   | 256 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 266 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |██████████████                  | 276 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 286 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |███████████████                 | 296 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |███████████████▍                | 307 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |████████████████                | 317 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |████████████████▌               | 327 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |█████████████████               | 337 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 348 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |██████████████████              | 358 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 368 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |███████████████████             | 378 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 389 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |████████████████████            | 399 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 409 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████           | 419 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 430 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 440 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 450 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 460 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 471 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 481 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 491 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 501 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 512 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 522 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 532 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 542 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 552 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 563 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 573 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 583 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 593 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 604 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 614 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 624 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 634 kB 9.8 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 636 kB 9.8 MB\/s \r\n",
            "\u001b[?25hCollecting nltk>=3.1\r\n",
            "  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\r\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 1.1 MB\/s eta 0:00:02\r\u001b[K     |▌                               | 20 kB 2.2 MB\/s eta 0:00:01\r\u001b[K     |▊                               | 30 kB 3.2 MB\/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 4.1 MB\/s eta 0:00:01\r\u001b[K     |█▏                              | 51 kB 5.0 MB\/s eta 0:00:01\r\u001b[K     |█▍                              | 61 kB 5.7 MB\/s eta 0:00:01\r\u001b[K     |█▋                              | 71 kB 4.6 MB\/s eta 0:00:01\r\u001b[K     |█▉                              | 81 kB 5.1 MB\/s eta 0:00:01\r\u001b[K     |██                              | 92 kB 5.7 MB\/s eta 0:00:01\r\u001b[K     |██▎                             | 102 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██▌                             | 112 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██▊                             | 122 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███                             | 133 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███▏                            | 143 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███▍                            | 153 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███▋                            | 163 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███▉                            | 174 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████                            | 184 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████▎                           | 194 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████▌                           | 204 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████▊                           | 215 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████                           | 225 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████▏                          | 235 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████▍                          | 245 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████▋                          | 256 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████▉                          | 266 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████                          | 276 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████▎                         | 286 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████▌                         | 296 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████▊                         | 307 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████                         | 317 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████▏                        | 327 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████▍                        | 337 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████▊                        | 348 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████                        | 358 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████▏                       | 368 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████▍                       | 378 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████▋                       | 389 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████▉                       | 399 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████                       | 409 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████▎                      | 419 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████▌                      | 430 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████▊                      | 440 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████                      | 450 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████▏                     | 460 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████▍                     | 471 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████▋                     | 481 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████▉                     | 491 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████                     | 501 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████▎                    | 512 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████▌                    | 522 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████▊                    | 532 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████                    | 542 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████▏                   | 552 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████▍                   | 563 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████▋                   | 573 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████▉                   | 583 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████████                   | 593 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 604 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 614 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 624 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████████                  | 634 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 645 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 655 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 665 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 675 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████████▏                | 686 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████████▍                | 696 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████████▋                | 706 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████████▉                | 716 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████████                | 727 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████████▎               | 737 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████████▌               | 747 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████████▊               | 757 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████████████               | 768 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 778 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 788 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 798 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 808 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████████████              | 819 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 829 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 839 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 849 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████████████             | 860 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 870 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 880 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 890 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 901 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████████████            | 911 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 921 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 931 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 942 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████           | 952 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 962 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 972 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 983 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 993 kB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.0 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.0 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.0 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.0 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.1 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.1 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.1 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.1 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.1 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.2 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.2 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.2 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.2 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.3 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.3 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.3 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.3 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.3 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.3 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.3 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.4 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.4 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.4 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.4 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.4 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.4 MB 6.2 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5 MB 6.2 MB\/s \r\n",
            "\u001b[?25hRequirement already satisfied: regex in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from nltk>=3.1->textblob) (2021.7.6)\r\n",
            "Requirement already satisfied: joblib in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from nltk>=3.1->textblob) (1.0.1)\r\n",
            "Requirement already satisfied: tqdm in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from nltk>=3.1->textblob) (4.61.2)\r\n",
            "Requirement already satisfied: click in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from nltk>=3.1->textblob) (7.1.2)\r\n",
            "Installing collected packages: nltk, textblob\r\n",
            "Successfully installed nltk-3.6.2 textblob-0.15.3\r\n",
            "\u001b[33mWARNING: You are using pip version 21.1; however, version 21.2.4 is available.\r\n",
            "You should consider upgrading via the '\/opt\/python\/envs\/default\/bin\/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
            "Requirement already satisfied: nltk in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (3.6.2)\r\n",
            "Requirement already satisfied: click in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from nltk) (7.1.2)\r\n",
            "Requirement already satisfied: tqdm in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from nltk) (4.61.2)\r\n",
            "Requirement already satisfied: joblib in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from nltk) (1.0.1)\r\n",
            "Requirement already satisfied: regex in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from nltk) (2021.7.6)\r\n",
            "\u001b[33mWARNING: You are using pip version 21.1; however, version 21.2.4 is available.\r\n",
            "You should consider upgrading via the '\/opt\/python\/envs\/default\/bin\/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
            "Collecting wordcloud\r\n",
            "  Downloading wordcloud-1.8.1-cp38-cp38-manylinux1_x86_64.whl (371 kB)\r\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 21.4 MB\/s eta 0:00:01\r\u001b[K     |█▊                              | 20 kB 24.0 MB\/s eta 0:00:01\r\u001b[K     |██▋                             | 30 kB 26.5 MB\/s eta 0:00:01\r\u001b[K     |███▌                            | 40 kB 29.8 MB\/s eta 0:00:01\r\u001b[K     |████▍                           | 51 kB 25.5 MB\/s eta 0:00:01\r\u001b[K     |█████▎                          | 61 kB 26.9 MB\/s eta 0:00:01\r\u001b[K     |██████▏                         | 71 kB 12.5 MB\/s eta 0:00:01\r\u001b[K     |███████                         | 81 kB 13.6 MB\/s eta 0:00:01\r\u001b[K     |████████                        | 92 kB 14.5 MB\/s eta 0:00:01\r\u001b[K     |████████▉                       | 102 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |█████████▊                      | 112 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |██████████▋                     | 122 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |███████████▌                    | 133 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |████████████▎                   | 143 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 153 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |██████████████                  | 163 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |███████████████                 | 174 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |███████████████▉                | 184 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |████████████████▊               | 194 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 204 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 215 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 225 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 235 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 245 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████          | 256 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████         | 266 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 276 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 286 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 296 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 307 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 317 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 327 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 337 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 348 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 358 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 368 kB 11.5 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 371 kB 11.5 MB\/s \r\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from wordcloud) (3.3.4)\r\n",
            "Requirement already satisfied: pillow in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from wordcloud) (8.3.1)\r\n",
            "Requirement already satisfied: numpy>=1.6.1 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from wordcloud) (1.19.5)\r\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from matplotlib->wordcloud) (2.4.7)\r\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from matplotlib->wordcloud) (1.3.1)\r\n",
            "Requirement already satisfied: python-dateutil>=2.1 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from matplotlib->wordcloud) (2.8.2)\r\n",
            "Requirement already satisfied: cycler>=0.10 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from matplotlib->wordcloud) (0.10.0)\r\n",
            "Requirement already satisfied: six in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\r\n",
            "Installing collected packages: wordcloud\r\n",
            "Successfully installed wordcloud-1.8.1\r\n",
            "\u001b[33mWARNING: You are using pip version 21.1; however, version 21.2.4 is available.\r\n",
            "You should consider upgrading via the '\/opt\/python\/envs\/default\/bin\/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
            "Collecting tweepy\r\n",
            "  Downloading tweepy-3.10.0-py2.py3-none-any.whl (30 kB)\r\n",
            "Requirement already satisfied: six>=1.10.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from tweepy) (1.15.0)\r\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from tweepy) (1.3.0)\r\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from tweepy) (2.25.1)\r\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.1)\r\n",
            "Requirement already satisfied: certifi>=2017.4.17 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from requests[socks]>=2.11.1->tweepy) (2021.5.30)\r\n",
            "Requirement already satisfied: idna<3,>=2.5 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\r\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from requests[socks]>=2.11.1->tweepy) (1.26.6)\r\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from requests[socks]>=2.11.1->tweepy) (4.0.0)\r\n",
            "Collecting PySocks!=1.5.7,>=1.5.6\r\n",
            "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\r\n",
            "Installing collected packages: PySocks, tweepy\r\n",
            "Successfully installed PySocks-1.7.1 tweepy-3.10.0\r\n",
            "\u001b[33mWARNING: You are using pip version 21.1; however, version 21.2.4 is available.\r\n",
            "You should consider upgrading via the '\/opt\/python\/envs\/default\/bin\/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
            "Collecting langdetect\r\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\r\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 24.7 MB\/s eta 0:00:01\r\u001b[K     |▊                               | 20 kB 26.3 MB\/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 12.5 MB\/s eta 0:00:01\r\u001b[K     |█▍                              | 40 kB 10.2 MB\/s eta 0:00:01\r\u001b[K     |█▊                              | 51 kB 10.9 MB\/s eta 0:00:01\r\u001b[K     |██                              | 61 kB 11.7 MB\/s eta 0:00:01\r\u001b[K     |██▍                             | 71 kB 10.8 MB\/s eta 0:00:01\r\u001b[K     |██▊                             | 81 kB 12.0 MB\/s eta 0:00:01\r\u001b[K     |███                             | 92 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███▍                            | 102 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███▊                            | 112 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |████                            | 122 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |████▍                           | 133 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |████▊                           | 143 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |█████                           | 153 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |█████▍                          | 163 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |█████▊                          | 174 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |██████                          | 184 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |██████▍                         | 194 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |██████▊                         | 204 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███████                         | 215 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███████▍                        | 225 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███████▊                        | 235 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |████████                        | 245 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |████████▍                       | 256 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |████████▊                       | 266 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |█████████                       | 276 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |█████████▍                      | 286 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |█████████▊                      | 296 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |██████████                      | 307 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |██████████▍                     | 317 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |██████████▊                     | 327 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███████████                     | 337 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███████████▍                    | 348 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███████████▊                    | 358 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |████████████                    | 368 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |████████████▍                   | 378 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |████████████▊                   | 389 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |█████████████                   | 399 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 409 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 419 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |██████████████                  | 430 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 440 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 450 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███████████████                 | 460 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███████████████▍                | 471 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███████████████▊                | 481 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |████████████████                | 491 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |████████████████▍               | 501 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |████████████████▊               | 512 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |█████████████████               | 522 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 532 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 542 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |██████████████████              | 552 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 563 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 573 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███████████████████             | 583 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 593 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 604 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |████████████████████            | 614 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 624 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 634 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 655 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 665 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 686 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 696 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 716 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 727 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 747 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 757 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 768 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 778 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 788 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 798 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 808 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 819 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 829 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 839 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 849 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 860 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 870 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 880 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 890 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 901 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 911 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 921 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 931 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 942 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 952 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 962 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 972 kB 11.0 MB\/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 981 kB 11.0 MB\/s \r\n",
            "\u001b[?25hRequirement already satisfied: six in \/opt\/python\/envs\/default\/lib\/python3.8\/site-packages (from langdetect) (1.15.0)\r\n",
            "Building wheels for collected packages: langdetect\r\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
            "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=65110696eb0c7b09817a04eb01192eee873455333edea7c447f71a2a43c3dbff\r\n",
            "  Stored in directory: \/home\/datalore\/.cache\/pip\/wheels\/13\/c7\/b0\/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\r\n",
            "Successfully built langdetect\r\n",
            "Installing collected packages: langdetect\r\n",
            "Successfully installed langdetect-1.0.9\r\n",
            "\u001b[33mWARNING: You are using pip version 21.1; however, version 21.2.4 is available.\r\n",
            "You should consider upgrading via the '\/opt\/python\/envs\/default\/bin\/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
          ],
          "output_type":"stream"
        },
        {
          "name":"stderr",
          "text":[
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     \/home\/datalore\/nltk_data...\n",
            "[nltk_data]   Unzipping corpora\/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to \/home\/datalore\/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to \/home\/datalore\/nltk_data...\n",
            "[nltk_data]   Unzipping corpora\/wordnet.zip.\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "stocks = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/benvictoria17\/DataVisualization\/master\/dataset\/FinancialTweets\/stocks_cleaned.csv\")\n",
        "Data = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/benvictoria17\/DataVisualization\/master\/dataset\/FinancialTweets\/stockerbot-export.csv\",error_bad_lines=False)"
      ],
      "execution_count":2,
      "outputs":[
        {
          "name":"stderr",
          "text":[
            "b'Skipping line 731: expected 8 fields, saw 13\\nSkipping line 2836: expected 8 fields, saw 15\\nSkipping line 3058: expected 8 fields, saw 12\\nSkipping line 3113: expected 8 fields, saw 12\\nSkipping line 3194: expected 8 fields, saw 17\\nSkipping line 3205: expected 8 fields, saw 17\\nSkipping line 3255: expected 8 fields, saw 17\\nSkipping line 3520: expected 8 fields, saw 17\\nSkipping line 4078: expected 8 fields, saw 17\\nSkipping line 4087: expected 8 fields, saw 17\\nSkipping line 4088: expected 8 fields, saw 17\\nSkipping line 4499: expected 8 fields, saw 12\\n'\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "Data.info()"
      ],
      "execution_count":3,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 28264 entries, 0 to 28263\n",
            "Data columns (total 8 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   id             28264 non-null  int64 \n",
            " 1   text           28264 non-null  object\n",
            " 2   timestamp      28264 non-null  object\n",
            " 3   source         28264 non-null  object\n",
            " 4   symbols        28264 non-null  object\n",
            " 5   company_names  28263 non-null  object\n",
            " 6   url            21895 non-null  object\n",
            " 7   verified       28264 non-null  bool  \n",
            "dtypes: bool(1), int64(1), object(6)\n",
            "memory usage: 1.5+ MB\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "'''Convert Columns data types '''\n",
        "\n",
        "# stockerbot[\"timestamp\"] = pd.to_datetime(stockerbot[\"timestamp\"])\n",
        "Data[\"text\"] = Data[\"text\"].astype(str)\n",
        "Data[\"url\"] = Data[\"url\"].astype(str)\n",
        "Data[\"company_names\"] = Data[\"company_names\"].astype(\"category\")\n",
        "Data[\"symbols\"] = Data[\"symbols\"].astype(\"category\")\n",
        "Data[\"source\"] = Data[\"source\"].astype(\"category\")"
      ],
      "execution_count":4,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "Data[['dayofweek','month','day','time','timezone', 'year']] = Data.timestamp.str.split(expand=True)\n",
        "Data[['hour','minute','second']] = Data.time.str.split(':',expand=True)\n",
        "Data.head(2)"
      ],
      "execution_count":5,
      "outputs":[
        {
          "data":{
            "text\/html":[
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "<\/style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th><\/th>\n",
              "      <th>id<\/th>\n",
              "      <th>text<\/th>\n",
              "      <th>timestamp<\/th>\n",
              "      <th>source<\/th>\n",
              "      <th>symbols<\/th>\n",
              "      <th>company_names<\/th>\n",
              "      <th>url<\/th>\n",
              "      <th>verified<\/th>\n",
              "      <th>dayofweek<\/th>\n",
              "      <th>month<\/th>\n",
              "      <th>day<\/th>\n",
              "      <th>time<\/th>\n",
              "      <th>timezone<\/th>\n",
              "      <th>year<\/th>\n",
              "      <th>hour<\/th>\n",
              "      <th>minute<\/th>\n",
              "      <th>second<\/th>\n",
              "    <\/tr>\n",
              "  <\/thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0<\/th>\n",
              "      <td>1019696670777503700<\/td>\n",
              "      <td>VIDEO: “I was in my office. I was minding my o...<\/td>\n",
              "      <td>Wed Jul 18 21:33:26 +0000 2018<\/td>\n",
              "      <td>GoldmanSachs<\/td>\n",
              "      <td>GS<\/td>\n",
              "      <td>The Goldman Sachs<\/td>\n",
              "      <td>https:\/\/twitter.com\/i\/web\/status\/1019696670777...<\/td>\n",
              "      <td>True<\/td>\n",
              "      <td>Wed<\/td>\n",
              "      <td>Jul<\/td>\n",
              "      <td>18<\/td>\n",
              "      <td>21:33:26<\/td>\n",
              "      <td>+0000<\/td>\n",
              "      <td>2018<\/td>\n",
              "      <td>21<\/td>\n",
              "      <td>33<\/td>\n",
              "      <td>26<\/td>\n",
              "    <\/tr>\n",
              "    <tr>\n",
              "      <th>1<\/th>\n",
              "      <td>1019709091038548000<\/td>\n",
              "      <td>The price of lumber $LB_F is down 22% since hi...<\/td>\n",
              "      <td>Wed Jul 18 22:22:47 +0000 2018<\/td>\n",
              "      <td>StockTwits<\/td>\n",
              "      <td>M<\/td>\n",
              "      <td>Macy's<\/td>\n",
              "      <td>https:\/\/twitter.com\/i\/web\/status\/1019709091038...<\/td>\n",
              "      <td>True<\/td>\n",
              "      <td>Wed<\/td>\n",
              "      <td>Jul<\/td>\n",
              "      <td>18<\/td>\n",
              "      <td>22:22:47<\/td>\n",
              "      <td>+0000<\/td>\n",
              "      <td>2018<\/td>\n",
              "      <td>22<\/td>\n",
              "      <td>22<\/td>\n",
              "      <td>47<\/td>\n",
              "    <\/tr>\n",
              "  <\/tbody>\n",
              "<\/table>\n",
              "<\/div>"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "Data.isnull().any()"
      ],
      "execution_count":6,
      "outputs":[
        {
          "data":{
            "text\/plain":[
              "id               False\n",
              "text             False\n",
              "timestamp        False\n",
              "source           False\n",
              "symbols          False\n",
              "company_names     True\n",
              "url              False\n",
              "verified         False\n",
              "dayofweek        False\n",
              "month            False\n",
              "day              False\n",
              "time             False\n",
              "timezone         False\n",
              "year             False\n",
              "hour             False\n",
              "minute           False\n",
              "second           False\n",
              "dtype: bool"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "print(f'null :{Data.company_names.isnull()}')\n",
        "Data[Data['company_names'].isnull()]"
      ],
      "execution_count":7,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "null :0        False\n",
            "1        False\n",
            "2        False\n",
            "3        False\n",
            "4        False\n",
            "         ...  \n",
            "28259    False\n",
            "28260    False\n",
            "28261    False\n",
            "28262    False\n",
            "28263    False\n",
            "Name: company_names, Length: 28264, dtype: bool\n"
          ],
          "output_type":"stream"
        },
        {
          "data":{
            "text\/html":[
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "<\/style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th><\/th>\n",
              "      <th>id<\/th>\n",
              "      <th>text<\/th>\n",
              "      <th>timestamp<\/th>\n",
              "      <th>source<\/th>\n",
              "      <th>symbols<\/th>\n",
              "      <th>company_names<\/th>\n",
              "      <th>url<\/th>\n",
              "      <th>verified<\/th>\n",
              "      <th>dayofweek<\/th>\n",
              "      <th>month<\/th>\n",
              "      <th>day<\/th>\n",
              "      <th>time<\/th>\n",
              "      <th>timezone<\/th>\n",
              "      <th>year<\/th>\n",
              "      <th>hour<\/th>\n",
              "      <th>minute<\/th>\n",
              "      <th>second<\/th>\n",
              "    <\/tr>\n",
              "  <\/thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3369<\/th>\n",
              "      <td>1017415512811135000<\/td>\n",
              "      <td>When you try to gauge sentiment on a $ticker b...<\/td>\n",
              "      <td>Thu Jul 12 14:28:55 +0000 2018<\/td>\n",
              "      <td>provotrout<\/td>\n",
              "      <td>ticker<\/td>\n",
              "      <td>NaN<\/td>\n",
              "      <td>nan<\/td>\n",
              "      <td>False<\/td>\n",
              "      <td>Thu<\/td>\n",
              "      <td>Jul<\/td>\n",
              "      <td>12<\/td>\n",
              "      <td>14:28:55<\/td>\n",
              "      <td>+0000<\/td>\n",
              "      <td>2018<\/td>\n",
              "      <td>14<\/td>\n",
              "      <td>28<\/td>\n",
              "      <td>55<\/td>\n",
              "    <\/tr>\n",
              "  <\/tbody>\n",
              "<\/table>\n",
              "<\/div>"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# Define Clean Function to fix text\n",
        "def Clean(text):\n",
        "\n",
        "  # Frist converting all letters to lower case\n",
        "  text= text.lower()\n",
        "  \n",
        "  # removing unwanted digits ,special chracters from the text\n",
        "  text= ' '.join(re.sub(\"(@[A-Za-z0-9]+)\", \" \", text).split()) #tags\n",
        "  text= ' '.join(re.sub(\"^@?(\\w){1,15}$\", \" \", text).split())\n",
        "    \n",
        "  text= ' '.join(re.sub(\"(\\w+:\\\/\\\/\\S+)\", \" \", text).split())   #Links\n",
        "  text= ' '.join(re.sub(\"http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\" \", text).split()) \n",
        "  text= ' '.join(re.sub(r'http\\S+', '',text).split())\n",
        "  \n",
        "  \n",
        "  text= ' '.join(re.sub(r'www\\S+', '',text).split())\n",
        "  text= ' '.join(re.sub(\"\\s+\", \" \",text).split()) #Extrem white Space\n",
        "  text= ' '.join(re.sub(\"[^-9A-Za-z ]\", \"\" ,text).split()) #digits \n",
        "  text= ' '.join(re.sub('-', ' ', text).split()) \n",
        "  text= ' '.join(re.sub('_', ' ', text).split()) #underscore \n",
        "  \n",
        "  # Display available PUNCTUATION for examples\n",
        "  #for c in string.punctuation:\n",
        "       #print(f\"[{c}]\")\n",
        "  \n",
        "  # removing stopwards and numbers from STRING library\n",
        "  table= str.maketrans('', '', string.punctuation+string.digits)\n",
        "  text = text.translate(table)\n",
        "  \n",
        "  # Split Sentence as tokens words \n",
        "  tokens = word_tokenize(text)\n",
        "  \n",
        "  # converting words to their root forms by STEMMING THE WORDS \n",
        "#   stemmed1 = [lemmatizer.lemmatize(word) for word in tokens] #Covert words to their actual root\n",
        "  stemmed2 = [porter.stem(word) for word in tokens] # Covert words to their rootbut not actual\n",
        "  \n",
        "  # Delete each stop words from English stop words\n",
        "#   words = [w for w in stemmed1 if not w in n_words] #n_words contains English stop words\n",
        "  words = [w for w in stemmed2 if not w in n_words] #n_words contains English stop words\n",
        "\n",
        "  text  = ' '.join(words)\n",
        "    \n",
        "  return text"
      ],
      "execution_count":8,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "Data.text"
      ],
      "execution_count":9,
      "outputs":[
        {
          "data":{
            "text\/plain":[
              "0        VIDEO: “I was in my office. I was minding my o...\n",
              "1        The price of lumber $LB_F is down 22% since hi...\n",
              "2        Who says the American Dream is dead? https:\/\/t...\n",
              "3        Barry Silbert is extremely optimistic on bitco...\n",
              "4        How satellites avoid attacks and space junk wh...\n",
              "                               ...                        \n",
              "28259           $FB : 29234a9c-7f08-4d5a-985f-cb1a5554ecf9\n",
              "28260    【仮想通貨】ビットコインの価格上昇、８０万円台回復　約１カ月半ぶり　　　　　　$BTC ht...\n",
              "28261    RT @invest_in_hd: 'Nuff said!  $TEL #telcoin #...\n",
              "28262    【仮想通貨】ビットコインの価格上昇、８０万円台回復　約１カ月半ぶり　　　　　　$BTC ht...\n",
              "28263    Stellar $XLM price: $0.297852 Binance registra...\n",
              "Name: text, Length: 28264, dtype: object"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "Data=Data[Data[\"source\"] != \"test5f1798\"]"
      ],
      "execution_count":10,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "Data.text=[Clean(x) for x in Data.text]\n",
        "# Delete Unwanted Some Text \n",
        "Data=Data[Data[\"text\"]!='btc']"
      ],
      "execution_count":11,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "Data.text"
      ],
      "execution_count":12,
      "outputs":[
        {
          "data":{
            "text\/plain":[
              "0        video wa offic wa mind busi david solomon tell...\n",
              "1        price lumber lbf sinc hit ytd high maci turnar...\n",
              "2                                  say american dream dead\n",
              "3        barri silbert extrem optimist bitcoin predict ...\n",
              "4        satellit avoid attack space junk circl earth paid\n",
              "                               ...                        \n",
              "28256    exxon onc perfect machin run dri wall street j...\n",
              "28257                         fallen hero today btc action\n",
              "28258    new exchang telcoin mid august im glad tel big...\n",
              "28261    inhd nuff said tel telcoin telfam crypto block...\n",
              "28263     stellar xlm price binanc registr open limit time\n",
              "Name: text, Length: 28225, dtype: object"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "\n",
        "Data=Data[Data['valid']==True]\n",
        "'''Detect languages for each text to filter into specific Lang'''\n",
        "\n",
        "languages = []\n",
        "\n",
        "# Loop over the sentences in the data and detect their language\n",
        "for row in range(len(Data)):\n",
        "    languages.append(detect_langs(Data.iloc[row, 0]))\n",
        "    \n",
        "# print('The detected languages are: ', languages) >>> ['en':'N']\n",
        "languages = [str(lang).split(':')[0][1:] for lang in languages] \n",
        "\n",
        "# Assign the list to a new feature \n",
        "Data['language'] = languages\n",
        "# look at Lang detected from our text\n",
        "\n",
        "Data['language'].value_counts()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "Data=Data[Data['language']=='en']\n",
        "Data=Data[['text','url','year','month','day','dayofweek','hour','minute','second','source','symbols','Polarity','Emotion','language','verified']]\n",
        "Data.head(4)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "\n",
        "def wordcloud_draw(data, color = 'black'):\n",
        "    words = ' '.join(data)\n",
        "    cleaned_word = \" \".join([word for word in words.split()\n",
        "                            if 'http' not in word  # double check for nay links\n",
        "                                and not word.startswith('#')  # removing hash tags\n",
        "                                and word != 'rt'  \n",
        "                            ])\n",
        "    wordcloud = WordCloud(stopwords=STOPWORDS, # using stopwords provided by Word cloud its optional since we already removed stopwords :)\n",
        "                      background_color=color,\n",
        "                      width=2500,\n",
        "                      height=2000\n",
        "                     ).generate(cleaned_word)\n",
        "    # using matplotlib to display the images in notebook itself.\n",
        "    plt.figure(1,figsize=(13, 13))\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "  \n",
        "# Percentage of each Emotions overall symbols\n",
        "\n",
        "df_neutral   = Data['text'][ Data['Emotion'] == '0']\n",
        "df_positive  = Data['text'][ Data['Emotion'] == '1']\n",
        "df_negative  = Data['text'][ Data['Emotion'] == '2']\n",
        "\n",
        "\n",
        "print(f' Percentage Positive: {len(df_positive)\/len(Data)}\\n Percentage Negetive: {len(df_negative)\/len(Data)}\\n Percentage Neutral: {len(df_neutral)\/len(Data)}')"
      ],
      "execution_count":14,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            " Percentage Positive: 0.2195571302037201\n",
            " Percentage Negetive: 0.07358724534986714\n",
            " Percentage Neutral: 0.7068556244464127\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def NgramModels(Model , txt, n):\n",
        "    \n",
        "    x_train, x_test, y_train, y_test = train_test_split(Data['text'], Data['Emotion'], test_size=0.2, random_state=50)\n",
        "    \n",
        "    vect      = CountVectorizer(max_features=1000 , ngram_range=(n,n))\n",
        "    train_vect= vect.fit_transform(x_train)\n",
        "    test_vect = vect.transform(x_test)\n",
        "    \n",
        "    model     = Model\n",
        "    t0        = time.time()\n",
        "    model.fit(train_vect, y_train)\n",
        "    t1        = time.time()\n",
        "    predicted = model.predict(test_vect)\n",
        "    t2        = time.time()\n",
        "    time_train= t1-t0\n",
        "    time_pred = t2-t1\n",
        "    \n",
        "    accuracy  = model.score(train_vect, y_train)\n",
        "    predicted = model.predict(test_vect)\n",
        "    \n",
        "    report = classification_report(y_test, predicted, output_dict=True)\n",
        "    print(\"Models with \" , n , \"-grams :\\n\")\n",
        "    print('********************** \\n')\n",
        "    print(txt)\n",
        "    print(\"Training time: %fs; Prediction time: %fs \\n\" % (time_train, time_pred))\n",
        "    print('Accuracy score train set :', accuracy)\n",
        "    print('Accuracy score test set  :', accuracy_score(y_test, predicted),'\\n')\n",
        "    print('Positive: ', report['1'])\n",
        "    print('Neutral : ', report['0'])\n",
        "    print('Negative: ', report['2'])\n",
        "    print('\\n --------------------------------------------------------------------------------------------------- \\n')\n",
        "def KNN_Ngram(n):\n",
        "    \n",
        "    x_train, x_test, y_train, y_test = train_test_split(Data['text'], Data['Emotion'], test_size=0.2, random_state=50)\n",
        "    \n",
        "    vect      = CountVectorizer(max_features=1000 , ngram_range=(n,n))\n",
        "    train_vect= vect.fit_transform(x_train)\n",
        "    test_vect = vect.transform(x_test)\n",
        "    \n",
        "    for k in [1,3,5,7,10]:\n",
        "\n",
        "        model = KNeighborsClassifier(n_neighbors=k,algorithm='brute')\n",
        "        t0        = time.time()\n",
        "        model.fit(train_vect, y_train)\n",
        "        t1        = time.time()\n",
        "        predicted = model.predict(test_vect)\n",
        "        t2        = time.time()\n",
        "        time_train= t1-t0\n",
        "        time_pred = t2-t1\n",
        "\n",
        "        accuracy  = model.score(train_vect, y_train)\n",
        "        predicted = model.predict(test_vect)\n",
        "\n",
        "        report = classification_report(y_test, predicted, output_dict=True)\n",
        "\n",
        "        print(\"Models with \" , n , \"-grams :\\n\")\n",
        "        print('********************** \\n')\n",
        "        print(\"Classification Report for k = {} is:\\n\".format(k))\n",
        "        print(\"Training time: %fs ; Prediction time: %fs \\n\" % (time_train, time_pred))\n",
        "        print('Accuracy score train set :', accuracy)\n",
        "        print('Accuracy score test set  :', accuracy_score(y_test, predicted),'\\n')\n",
        "        print('Positive: ', report['1'])\n",
        "        print('Neutral : ', report['0'])\n",
        "        print('Negative: ', report['2'])\n",
        "        print('\\n -------------------------------------------------------------------------------------- \\n')\n",
        "def TFIDFModels(Model,txt):\n",
        "    \n",
        "    x_train, x_test, y_train, y_test = train_test_split(Data['text'], Data['Emotion'], test_size=0.2, random_state=50)\n",
        "    \n",
        "    vect      = TfidfVectorizer(min_df = 5, max_df =0.8, sublinear_tf = True, use_idf = True)\n",
        "    train_vect= vect.fit_transform(x_train)\n",
        "    test_vect = vect.transform(x_test)\n",
        "    \n",
        "    model     = Model\n",
        "    t0        = time.time()\n",
        "    model.fit(train_vect, y_train)\n",
        "    t1        = time.time()\n",
        "    predicted = model.predict(test_vect)\n",
        "    t2        = time.time()\n",
        "    time_train= t1-t0\n",
        "    time_pred = t2-t1\n",
        "    \n",
        "    accuracy  = model.score(train_vect, y_train)\n",
        "    predicted = model.predict(test_vect)\n",
        "    \n",
        "    report = classification_report(y_test, predicted, output_dict=True)\n",
        "    \n",
        "    print(txt)\n",
        "    print(\"Training time: %fs; Prediction time: %fs \\n\" % (time_train, time_pred))\n",
        "    print('Accuracy score train set :', accuracy)\n",
        "    print('Accuracy score test set  :', accuracy_score(y_test, predicted),'\\n')\n",
        "    print('Positive: ', report['1'])\n",
        "    print('Neutral : ', report['0'])\n",
        "    print('Negative: ', report['2'])\n",
        "    print('\\n -------------------------------------------------------------------------------------- \\n')\n",
        "def KNN_TFIDF():\n",
        "    \n",
        "    x_train, x_test, y_train, y_test = train_test_split(Data['text'], Data['Emotion'], test_size=0.2, random_state=50)\n",
        "    \n",
        "    vect      = TfidfVectorizer(min_df = 5, max_df =0.8, sublinear_tf = True, use_idf = True)\n",
        "    train_vect= vect.fit_transform(x_train)\n",
        "    test_vect = vect.transform(x_test)\n",
        "    \n",
        "    for k in [1,3,5,7,10]:\n",
        "\n",
        "        model = KNeighborsClassifier(n_neighbors=k,algorithm='brute')\n",
        "        t0        = time.time()\n",
        "        model.fit(train_vect, y_train)\n",
        "        t1        = time.time()\n",
        "        predicted = model.predict(test_vect)\n",
        "        t2        = time.time()\n",
        "        time_train= t1-t0\n",
        "        time_pred = t2-t1\n",
        "\n",
        "        accuracy  = model.score(train_vect, y_train)\n",
        "        predicted = model.predict(test_vect)\n",
        "\n",
        "        report = classification_report(y_test, predicted, output_dict=True)\n",
        "\n",
        "        print(\"Classification Report for k = {} is:\\n\".format(k))\n",
        "        print(\"Training time: %fs ; Prediction time: %fs \\n\" % (time_train, time_pred))\n",
        "        print('Accuracy score train set :', accuracy)\n",
        "        print('Accuracy score test set  :', accuracy_score(y_test, predicted),'\\n')\n",
        "        print('Positive: ', report['1'])\n",
        "        print('Neutral : ', report['0'])\n",
        "        print('Negative: ', report['2'])\n",
        "        print('\\n -------------------------------------------------------------------------------------- \\n')\n",
        "SupportVectorClassifier=svm.SVC(kernel='linear')\n",
        "\n",
        "LogReg2=NgramModels(Model=LogisticRegression(),txt='Logistic Regression Model : \\n ', n=2)\n",
        "LogReg3=NgramModels(Model=LogisticRegression(),txt='Logistic Regression Model : \\n ', n=3)\n",
        "\n",
        "svm2=NgramModels(Model=SupportVectorClassifier ,txt='Support Vectoer Classifier Model : \\n ', n=2)\n",
        "svm3=NgramModels(Model=SupportVectorClassifier ,txt='Support Vectoer Classifier Model : \\n ', n=3)\n",
        "\n",
        "DecTree2=NgramModels(Model=tree.DecisionTreeClassifier(),txt='Decision Tree Classifier Model : \\n ', n=2)\n",
        "DecTree3=NgramModels(Model=tree.DecisionTreeClassifier(),txt='Decision Tree Classifier Model : \\n ', n=3)\n",
        "\n",
        "KNN2=KNN_Ngram(2)\n",
        "KNN3=KNN_Ngram(3)"
      ],
      "execution_count":0,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "Models with  2 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Logistic Regression Model : \n",
            " \n",
            "Training time: 0.995938s; Prediction time: 0.000499s \n",
            "\n",
            "Accuracy score train set : 0.8269264836138175\n",
            "Accuracy score test set  : 0.8278122232063774 \n",
            "\n",
            "Positive:  {'precision': 0.9207207207207208, 'recall': 0.4205761316872428, 'f1-score': 0.5774011299435028, 'support': 1215}\n",
            "Neutral :  {'precision': 0.8143639683505782, 'recall': 0.9908664527277216, 'f1-score': 0.8939866369710469, 'support': 4051}\n",
            "Negative:  {'precision': 0.9192546583850931, 'recall': 0.39050131926121373, 'f1-score': 0.5481481481481482, 'support': 379}\n",
            "\n",
            " --------------------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  3 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Logistic Regression Model : \n",
            " \n",
            "Training time: 0.951523s; Prediction time: 0.000412s \n",
            "\n",
            "Accuracy score train set : 0.7986713906111603\n",
            "Accuracy score test set  : 0.8033658104517272 \n",
            "\n",
            "Positive:  {'precision': 0.9841269841269841, 'recall': 0.30617283950617286, 'f1-score': 0.4670433145009416, 'support': 1215}\n",
            "Neutral :  {'precision': 0.7868948084775423, 'recall': 0.999012589484078, 'f1-score': 0.8803567544050468, 'support': 4051}\n",
            "Negative:  {'precision': 0.9354838709677419, 'recall': 0.30606860158311344, 'f1-score': 0.46123260437375746, 'support': 379}\n",
            "\n",
            " --------------------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  2 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Support Vectoer Classifier Model : \n",
            " \n",
            "Training time: 7.860759s; Prediction time: 0.779209s \n",
            "\n",
            "Accuracy score train set : 0.8276350752878654\n",
            "Accuracy score test set  : 0.8295837023914969 \n",
            "\n",
            "Positive:  {'precision': 0.9479553903345725, 'recall': 0.41975308641975306, 'f1-score': 0.5818596691386194, 'support': 1215}\n",
            "Neutral :  {'precision': 0.8146272285251216, 'recall': 0.992594421130585, 'f1-score': 0.8948481139423612, 'support': 4051}\n",
            "Negative:  {'precision': 0.8888888888888888, 'recall': 0.40105540897097625, 'f1-score': 0.5527272727272727, 'support': 379}\n",
            "\n",
            " --------------------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  3 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Support Vectoer Classifier Model : \n",
            " \n",
            "Training time: 6.269881s; Prediction time: 0.990418s \n",
            "\n",
            "Accuracy score train set : 0.7984056687333924\n",
            "Accuracy score test set  : 0.8044286979627989 \n",
            "\n",
            "Positive:  {'precision': 0.9868421052631579, 'recall': 0.30864197530864196, 'f1-score': 0.4702194357366771, 'support': 1215}\n",
            "Neutral :  {'precision': 0.7870478413068844, 'recall': 0.999012589484078, 'f1-score': 0.8804525182203851, 'support': 4051}\n",
            "Negative:  {'precision': 0.967479674796748, 'recall': 0.31398416886543534, 'f1-score': 0.4741035856573706, 'support': 379}\n",
            "\n",
            " --------------------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  2 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Decision Tree Classifier Model : \n",
            " \n",
            "Training time: 0.254006s; Prediction time: 0.006143s \n",
            "\n",
            "Accuracy score train set : 0.8396811337466785\n",
            "Accuracy score test set  : 0.8276350752878654 \n",
            "\n",
            "Positive:  {'precision': 0.875, 'recall': 0.44938271604938274, 'f1-score': 0.5938009787928221, 'support': 1215}\n",
            "Neutral :  {'precision': 0.8197194719471947, 'recall': 0.9809923475685016, 'f1-score': 0.8931340600067423, 'support': 4051}\n",
            "Negative:  {'precision': 0.8786127167630058, 'recall': 0.40105540897097625, 'f1-score': 0.5507246376811594, 'support': 379}\n",
            "\n",
            " --------------------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  3 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Decision Tree Classifier Model : \n",
            " \n",
            "Training time: 0.159800s; Prediction time: 0.005689s \n",
            "\n",
            "Accuracy score train set : 0.8011071744906997\n",
            "Accuracy score test set  : 0.8026572187776794 \n",
            "\n",
            "Positive:  {'precision': 0.9640102827763496, 'recall': 0.30864197530864196, 'f1-score': 0.46758104738154616, 'support': 1215}\n",
            "Neutral :  {'precision': 0.7871345029239766, 'recall': 0.9967909158232535, 'f1-score': 0.8796427404422175, 'support': 4051}\n",
            "Negative:  {'precision': 0.9365079365079365, 'recall': 0.3113456464379947, 'f1-score': 0.4673267326732674, 'support': 379}\n",
            "\n",
            " --------------------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  2 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Classification Report for k = 1 is:\n",
            "\n",
            "Training time: 0.022100s ; Prediction time: 3.740794s \n",
            "\n",
            "Accuracy score train set : 0.6639503985828167\n",
            "Accuracy score test set  : 0.6453498671390611 \n",
            "\n",
            "Positive:  {'precision': 0.37443241355221796, 'recall': 0.8823045267489712, 'f1-score': 0.525747915644924, 'support': 1215}\n",
            "Neutral :  {'precision': 0.9386810644041651, 'recall': 0.6008392989385337, 'f1-score': 0.732691149909693, 'support': 4051}\n",
            "Negative:  {'precision': 0.7248677248677249, 'recall': 0.36147757255936674, 'f1-score': 0.4823943661971832, 'support': 379}\n",
            "\n",
            " -------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  2 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Classification Report for k = 3 is:\n",
            "\n",
            "Training time: 0.031826s ; Prediction time: 2.951987s \n",
            "\n",
            "Accuracy score train set : 0.8219220549158548\n",
            "Accuracy score test set  : 0.8152347209920283 \n",
            "\n",
            "Positive:  {'precision': 0.8316993464052288, 'recall': 0.4189300411522634, 'f1-score': 0.5571975916803503, 'support': 1215}\n",
            "Neutral :  {'precision': 0.8101472995090017, 'recall': 0.9775364107627746, 'f1-score': 0.8860051459894842, 'support': 4051}\n",
            "Negative:  {'precision': 0.9172413793103448, 'recall': 0.35092348284960423, 'f1-score': 0.5076335877862596, 'support': 379}\n",
            "\n",
            " -------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  2 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Classification Report for k = 5 is:\n",
            "\n",
            "Training time: 0.021843s ; Prediction time: 2.893387s \n",
            "\n",
            "Accuracy score train set : 0.8190434012400354\n",
            "Accuracy score test set  : 0.8177147918511958 \n",
            "\n",
            "Positive:  {'precision': 0.8763250883392226, 'recall': 0.4082304526748971, 'f1-score': 0.5569904548006738, 'support': 1215}\n",
            "Neutral :  {'precision': 0.8074974670719351, 'recall': 0.9837077264872871, 'f1-score': 0.8869352325840196, 'support': 4051}\n",
            "Negative:  {'precision': 0.9375, 'recall': 0.3562005277044855, 'f1-score': 0.5162523900573613, 'support': 379}\n",
            "\n",
            " -------------------------------------------------------------------------------------- \n",
            "\n",
            "Models with  2 -grams :\n",
            "\n",
            "********************** \n",
            "\n",
            "Classification Report for k = 7 is:\n",
            "\n",
            "Training time: 0.021817s ; Prediction time: 2.914513s \n",
            "\n",
            "Accuracy score train set : 0.8170062001771479\n",
            "Accuracy score test set  : 0.816829052258636 \n",
            "\n",
            "Positive:  {'precision': 0.898876404494382, 'recall': 0.3950617283950617, 'f1-score': 0.548885077186964, 'support': 1215}\n",
            "Neutral :  {'precision': 0.8064516129032258, 'recall': 0.9874105159219946, 'f1-score': 0.8878037953612251, 'support': 4051}\n",
            "Negative:  {'precision': 0.8675496688741722, 'recall': 0.34564643799472294, 'f1-score': 0.4943396226415095, 'support': 379}\n",
            "\n",
            " -------------------------------------------------------------------------------------- \n",
            "\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        
      }
    }
  ],
  "metadata":{
    
  },
  "nbformat":4,
  "nbformat_minor":0
}