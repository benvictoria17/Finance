{"cells":[{"metadata":{"trusted":true,"_uuid":"44d4fd1cfaf7363b4743005e994881170b8bd075"},"cell_type":"code","source":"## Importing packages\n\n# This R environment comes with all of CRAN and many other helpful packages preinstalled.\n# You can see which packages are installed by checking out the kaggle/rstats docker image: \n# https://github.com/kaggle/docker-rstats\n\n# Input files' directory\n# print(\"Input files: \")\n# list.files(path = \"../input\")\n\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(tidyr)\nlibrary(lubridate)\nlibrary(corrplot)\nlibrary(mice)\nlibrary(DMwR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3e94bc6fb4fc6b65e48903072d759a598235f40"},"cell_type":"code","source":"#----------------- ZILLOW'S HOME VALUE ESTIMATE (via LINEAR/MULTIPLE REGRESSION METHOD) ---------------\n\n# We are going to perform EDA on the Zillow's Home Value Estimate. \n# The following are good steps to begin the process:\n# 1. Import the data into a variable. (Load train_201 from .../input folder) \n# 2. Check the dimensions of the \nzillow_prop_2017 <- fread(\"../input/properties_2017.csv\")\npaste(\"Dimensions of complete_df: \" , toString(dim(zillow_prop_2017)))\n\nzillow_dataframe_2017 <- fread(\"../input/train_2017.csv\")\npaste(\"Dimensions of training_samples: \" , toString(dim(zillow_dataframe_2017)))\n\n# We have decided to go ahead with 2017 data because it being recent. We will further investigate `train_2017.csv` and `properties_2017`\n# to get more insight to the data.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffd3474589119dcae92f4337758ac859fff4a84b"},"cell_type":"code","source":"# We will look at the couple of head data to know the data better. \n# Obs: We can observe that the `train_2017.csv` file contains the training data \n#      with mentioned logerror and transaction data, while the `properties_2017.csv`\n#      contains the records and its attributes for training the model.\n\n#      We will need to club/merge both the files (natural_join) to get our training data. \n#      (We will do that after cleaning the data)\n\nhead(zillow_dataframe_2017, n=15)\nhead(zillow_prop_2017, n=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"684f6fb7d8dfb3e6099e49d5f6e83ca94f2af8f0"},"cell_type":"code","source":"# To clean the data, let us first rename the column names to our convenience.\n# \nzillow_prop_2017 <- zillow_prop_2017 %>% rename(\n  id_parcel =  parcelid,\n  build_year =  yearbuilt,\n  area_basement =  basementsqft,\n  area_patio =  yardbuildingsqft17,\n  area_shed =  yardbuildingsqft26, \n  area_pool =  poolsizesum,  \n  area_lot =  lotsizesquarefeet, \n  area_garage =  garagetotalsqft,\n  area_firstfloor_finished =  finishedfloor1squarefeet,\n  area_total_calc =  calculatedfinishedsquarefeet,\n  area_base =  finishedsquarefeet6,\n  area_live_finished =  finishedsquarefeet12,\n  area_liveperi_finished =  finishedsquarefeet13,\n  area_total_finished =  finishedsquarefeet15,  \n  area_unknown =  finishedsquarefeet50,\n  num_unit =  unitcnt, \n  num_story =  numberofstories,  \n  num_room =  roomcnt,\n  num_bathroom =  bathroomcnt,\n  num_bedroom =  bedroomcnt,\n  num_bathroom_calc =  calculatedbathnbr,\n  num_bath =  fullbathcnt,  \n  num_75_bath =  threequarterbathnbr, \n  num_fireplace =  fireplacecnt,\n  num_pool =  poolcnt,  \n  num_garage =  garagecarcnt,  \n  region_county =  regionidcounty,\n  region_city =  regionidcity,\n  region_zip =  regionidzip,\n  region_neighbor =  regionidneighborhood,  \n  tax_total =  taxvaluedollarcnt,\n  tax_building =  structuretaxvaluedollarcnt,\n  tax_land =  landtaxvaluedollarcnt,\n  tax_property =  taxamount,\n  tax_year =  assessmentyear,\n  tax_delinquency =  taxdelinquencyflag,\n  tax_delinquency_year =  taxdelinquencyyear,\n  zoning_property =  propertyzoningdesc,\n  zoning_landuse =  propertylandusetypeid,\n  zoning_landuse_county =  propertycountylandusecode,\n  flag_fireplace =  fireplaceflag, \n  flag_tub =  hashottuborspa,\n  quality =  buildingqualitytypeid,\n  framing =  buildingclasstypeid,\n  material =  typeconstructiontypeid,\n  deck =  decktypeid,\n  story =  storytypeid,\n  heating =  heatingorsystemtypeid,\n  aircon =  airconditioningtypeid,\n  architectural_style = architecturalstyletypeid\n)\n\ncolumnName_df <- colnames(zillow_prop_2017)\ncolumnName_df\n\nzillow_dataframe_2017 <- zillow_dataframe_2017 %>% rename(\n  id_parcel = parcelid,\n  date = transactiondate\n)\n\ncolumnName_train <- colnames(zillow_dataframe_2017)\ncolumnName_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0da3969feec259a6d82d9728b1ce7bed13d9f866"},"cell_type":"code","source":"# We will join the data from both the files to get a training dataset.\ncomplete_training_data <- zillow_dataframe_2017 %>% inner_join(zillow_prop_2017, by=\"id_parcel\")\ncomplete_training_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94ea5ad78800e858ee7512b5471515db1eaa23cd"},"cell_type":"code","source":"# On renaming the columns we are now a little comfortable with the data. \n# We will now find the percent of missing entries in all the columns \n# and then use the columns with less than 40% of missing data.\n# Reason: The graph clearly shows that we do not need the data\n# less than 40%. It reduces our number of columns significantly.\n\n#Finding null values \nnull_data_percent <- map_dbl(complete_training_data, function(x) { round((sum(is.na(x)) / length(x)) * 100, 2) })\ndata.frame(percent = null_data_percent, var = names(null_data_percent), row.names = NULL) %>%\n    ggplot(aes(x = reorder(var, -percent), y = percent)) +\n    geom_bar(stat = 'identity', fill = 'blue') +\n    labs(x = '', y = 'Missing Data %', title = '% missing data by all feature') +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n#Finding <40% null values\nnull_data_percent_more_than_40 <- null_data_percent[null_data_percent > 40]\nnull_data_percent_less_than_40 <- null_data_percent[null_data_percent <= 40]\n\ndata.frame(percent = null_data_percent_less_than_40, var = names(null_data_percent_less_than_40), row.names = NULL) %>%\n    ggplot(aes(x = reorder(var, -percent), y = percent)) +\n    geom_bar(stat = 'identity', fill = '#4169E1') +\n    labs(x = '', y = 'Missing Data %', title = '% missing data by feature (less than 40%)') +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\nnames(null_data_percent_less_than_40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"672e050a83008c299240e2e70d0f21939b0c4616"},"cell_type":"code","source":"#removing the columns with more than 40% missing data. \ncomplete_training_data[,c(names(null_data_percent_more_than_40))] <- NULL\npaste(\"Dimensions of df: \" , toString(dim(zillow_prop_2017)))\n\n# complete_training_data_clean <- na.exclude(complete_training_data)\n# dim(complete_training_data_clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15b7c77cafe8b2e09838b634171931f3e808cadb"},"cell_type":"code","source":"#To save the checkpoint for the dataframe and aviod running the program again and again.\ntotal_training <- complete_training_data\ntotal_training","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22d1d546c7edcb6135404d0dee5d02be01cc408a"},"cell_type":"code","source":"date_col <- as_datetime((total_training$date))\ntmp <- total_training %>% mutate(year_month = make_date(year = year(date_col), month = month(date_col)))\ntmp %>% \n  group_by(year_month) %>% count() %>% \n  ggplot(aes(x = year_month,y = n)) +\n  geom_bar( stat=\"identity\", fill=\"#6A5ACD\") +\n  geom_vline(aes(xintercept = as.numeric(as.Date(\"2016-10-01\"))),size=2) +\n  labs(x = 'Month-Year', y = 'Count', title = 'Count of Records v/s Time Graph')\n\n# This shows that there is more empty data than the needed data, and does not have much impact on the final result.\ntotal_training %>%\n  group_by(total_training$tax_delinquency) %>% count()\ntotal_training$tax_delinquency <- NULL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d833e47f49e87cc549b2f786154882e010b2f9b"},"cell_type":"code","source":"# Changing the string `date` column to type `Date`\nmapStringToDate <- function(stringDate) {\n  parse_date_time(stringDate, orders = c(\"ymd\"))\n}\ntotal_training$date <- sapply(total_training$date, mapStringToDate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"842a00841bfeffda146532e29e774ae2e9a7326a"},"cell_type":"code","source":"# We looked up a lot of kernels for managing the null values. \n# This is the best that we could come up for handling null values.\n\n# Replacing the null values with zero, this is not the best technique. \n# But the data is categorised anyway, and it could have been replaced using KNN techniques\n# but we have categoried it as 0 instead.\ncategories_zoning_landing_county <- unique(total_training$zoning_landuse_county)\nzoning_landing_county_col <- total_training$zoning_landuse_county\nfor (x in 1:length(zoning_landing_county_col)) {\n  temp <- match(zoning_landing_county_col[x], categories_zoning_landing_county)\n  if (is.na(temp)) {\n    temp <- 0\n  }\n  zoning_landing_county_col[x] = temp\n}\ntotal_training$zoning_landuse_county <- as.numeric(zoning_landing_county_col)\n\n#same goes heree\ncategories_zoning_property <- unique(total_training$zoning_property)\nzoning_property_col <- total_training$zoning_property\nfor (x in 1:length(zoning_property_col)) {\n  temp <- match(zoning_property_col[x], categories_zoning_property)\n  if (is.na(temp)) {\n    temp <- 0\n  }\n  zoning_property_col[x] = temp\n}\ntotal_training$zoning_property <- as.numeric(zoning_property_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"546099e3b47af4732a557b612260c6f3534789c4"},"cell_type":"code","source":"# Every id_parcel is linked to location information. There are plenty of geolocation data attributes that can be used.\n# These features can be condensed to form clusters - 'latitude', 'longitude', 'fips', 'regionidcounty', 'regionidcity', 'regionidzip', 'regionidneighborhood'\n\ngeographic_attributes <- c(\"latitude\", \"longitude\", \"fips\", \"region_county\", \"region_city\", \"region_zip\")\n\n# summary\ntotal_training %>% \n  select_(.dots = geographic_attributes) %>% \n  summary()\n\n# On further investigation, we can fill the NA's (as they are same number in latitude, longitude, fips,region_county). \n# First, we will re-create the original latitude & longitude by multipling it by 10e6.\n\ntotal_training$latitude = total_training$latitude/1000000\ntotal_training$longitude = total_training$longitude/1000000\n\n# In addition to that, we know that latitude and longitude are a unique combination. We will keep the location point\n# that has minimum null entries in the location attributes.\n\n# Create unique location point of lat_long\ntotal_training$pos = paste0(total_training$longitude,\"_\", total_training$latitude)\n\n# Subset the location attributes from total_training dataframe\ngeolocation.dt <- setDT(total_training[, c(geographic_attributes, \"pos\")])\n\n# Keep rows that contain most complete lat&long information\n# Calculate for each row the number of non-missing values (count) and then \n# just keep the rows where \"count\"\" is equal to the max for that group.\ncoordinates = copy(geolocation.dt)\ncoordinates = coordinates[, count := rowSums(!is.na(geolocation.dt))]\ncoordinates = coordinates[ , max.count := max(count, na.rm = TRUE), \n                           by = \"pos\"][count == max.count,.(region_county, region_city, region_zip, latitude, longitude, pos)]\n\ncoordinates = as.data.frame(coordinates)\ncoordinates = coordinates [!duplicated(coordinates$pos),]\n\n# Next, we will remove location attributes from the total_training dataframe and join with the coordinates data:\n# Remove location attributes, except latitude and longitude\ntotal_training = setDT(total_training)\ntotal_training[ , ':=' (region_county = NULL, \n                         region_city = NULL, \n                         region_zip = NULL,\n                         pos = NULL),]\ncoordinates = setDT(coordinates)\n\n# Merge with the coordinates data\ntotal_training = coordinates[total_training, ,on = c(\"latitude\", \"longitude\")]\ntotal_training = as.data.frame(total_training)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4b9f166f740f20fe598c9744c6d013b26b98d55"},"cell_type":"code","source":"# Now that we have a more reliable geo-location information, we will next replace the tails of the lat & long distribution with means for the whole dataset: \n\n#  Find longitude distribution tails \nlongitude    <- total_training$longitude\noutliers_lon <- quantile(longitude, c(0.001,0.999), na.rm = TRUE)\nxlon         <- which(!(longitude > outliers_lon[1] & longitude < outliers_lon[2]))\n\n# Find latitude distribution tails \nlatitude     <- total_training$latitude\noutliers_lat <- quantile(latitude, c(0.001,0.999), na.rm = TRUE)\nxlat         <- which(!(latitude > outliers_lat[1] & latitude < outliers_lat[2]))\n\n# Replace outliers with means\ntotal_training[xlat, c(\"latitude\")]  <- c(mean(total_training$latitude))\ntotal_training[xlon, c(\"longitude\")] <- c(mean(total_training$longitude))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"daa681a4f827c154d8828b0f6467aaabe0603d9d"},"cell_type":"code","source":"# We´ll do the same for records where we have missing latitude and longitude:\n\n# Replace missings with means\ntotal_training[is.na(total_training$latitude), c(\"latitude\")]   <- mean(total_training$latitude, na.rm = TRUE)\ntotal_training[is.na(total_training$longitude), c(\"longitude\")] <- mean(total_training$longitude, na.rm = TRUE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f6778c38cc93a1bf53b46d688fafa986a0fc7e9"},"cell_type":"code","source":"# Now, the last thing on geo-location attributes is clustering.\n\nlat_long <- total_training[,c(\"longitude\",\"latitude\")]\ncl <- kmeans(lat_long, length(unique(total_training$region_zip)))\ntotal_training[,\"kmeans_cluster\"] <- cl$cluster","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b1c5092ae6425eeff7f02c0b56034be1376d41b"},"cell_type":"code","source":"names(total_training)\ntotal_training","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f07fcdb1124336fa943b38129d96e52cd51f8ba"},"cell_type":"code","source":"# Now that we know about the location attributes, we can safely remove it and reduce the data.\ntotal_training[,c(\"pos\", \"fips\", \"region_county\", \"region_city\", \"region_zip\")] <- NULL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5501eefc762b92b967c6041eeb9142009c5a0f7"},"cell_type":"code","source":"summary(total_training)\npaste(\"Dimensions of training_samples: \" , toString(dim(total_training)))\n# We can easily do na.execlude() and the data will reduce drastically, but that also means data loss. Which is not a desired thing. \n# Let us see that happens in that. \n\nsummary(na.exclude(total_training))\npaste(\"Dimensions of training_samples: \" , toString(dim(na.exclude(total_training))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"936729cc7b0fc528fb6aa1c8dd12b46e290455b5"},"cell_type":"code","source":"# Using the above summary, we can conclude that na.exclude() removes a lot of data, that might be wanted for our use. \n# First let us set null to 0 because the string columns are taken care of. Only numeric (double and integer) columns are left.\ntotal_training[is.na(total_training)] <- 0\n\n# Thus we will inspect each attribute individually.\n# 1. lattitude - No change\n# 2. longitude - No change\n# 3. id_parcel - unique ID but with exception to 200 records that have duplicated id_parcel. There is no NA values\nlength(total_training$id_parcel)\nlength(unique(total_training$id_parcel))\n\n# 4. logerror - log error (does not have any NA values)\n# 5. date - converted the string to Date data type (earlier only)\n# 6. num_bathroom and num_bathroom_calc - show same description and also same statistics \n#     The data varies from 1 to 18. Increases suddenly in the 4th quartile (outliers)\n#     (We will remove the one with maximum NAs)\ntotal_training$num_bathroom_calc <- NULL\n# 7. Censustractandblock and rawcensustractandblock are ID <- remove\ntotal_training$censustractandblock <- NULL\ntotal_training$rawcensustractandblock <- NULL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"288c709f200378b04e8f828a1718628df8f02bc6"},"cell_type":"code","source":"# 8. build_year and tax_year can be taken care of by subtracting 2019 from it to normalise it.\ntotal_training$build_year <- 2019 - total_training$build_year\ntotal_training$tax_year <- 2019 - total_training$tax_year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc0f2de463933ef103a651384ae36b4d0d474fb0"},"cell_type":"code","source":"# We have condensed the data considerably. We can now focus only on the data and its attributes\n# that have strong correlation with each other. This will help in the linear regression analysis.\n# \ncorrelationMatrix <- as.data.frame(corrplot(cor(total_training, use=\"complete.obs\"), type=\"lower\"))\n\n# Thus we will now only keep the data with close relation to get good linear regression fit. \nx <- row.names(correlationMatrix)[abs(correlationMatrix$tax_property) > 0.50]\ny <- row.names(correlationMatrix)[abs(correlationMatrix$num_bath) > 0.50]\nz <- row.names(correlationMatrix)[abs(correlationMatrix$tax_building) > 0.50]\na <- row.names(correlationMatrix)[abs(correlationMatrix$tax_land) > 0.50]\nb <- row.names(correlationMatrix)[abs(correlationMatrix$quality) > 0.50]\nc <- row.names(correlationMatrix)[abs(correlationMatrix$num_room) > 0.50]\nd <- row.names(correlationMatrix)[abs(correlationMatrix$longitude) > 0.50]\n\n# Removing other columns to null them.\ncolumn_names_to_null <- base::setdiff(names(total_training), unique(union_all(a,b,c,d,x,y,z)))\ndummy <- total_training\ntotal_training[,c(column_names_to_null)] <- NULL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25495350892a6e621d11cc8314eccef65050a81b"},"cell_type":"code","source":"# Normalising Tax_building\ntotal_training$tax_building <- (total_training$tax_building - mean(total_training$tax_building)) / sd(total_training$tax_building)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cd975314d151bd8e596dae28ec0d71bf09c1970"},"cell_type":"code","source":"# Normalising Tax_land\ntotal_training$tax_total <- (total_training$tax_total - mean(total_training$tax_total)) / sd(total_training$tax_total)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd577f4a83a4c6c988030480226ef78e3833a9a7"},"cell_type":"code","source":"# Normalising Tax_land\ntotal_training$tax_land <- (total_training$tax_land - mean(total_training$tax_land)) / sd(total_training$tax_land)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7050f5f22d470567916ae0f6564da212fbbe70f5"},"cell_type":"code","source":"# Normalising Tax_property\ntotal_training$tax_property <- (total_training$tax_property - mean(total_training$tax_property)) / sd(total_training$tax_property)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ef5a44fa06d7fb51f578ea2a9479bdc8135ee6a"},"cell_type":"code","source":"# Creating model for making linear model - Tax Attributes\nmodel1 <- lm(tax_building ~ tax_land + tax_property + tax_total, total_training)\nprint(model1)\nsummary(model1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a235f5fbfb9cbe5e6313b921523ee96321450b8"},"cell_type":"code","source":"# Creating model for making linear model - Room-Features Attributes\nmodel2 <- lm(quality ~ area_total_calc + area_live_finished + num_bath + num_room + num_bathroom + num_bedroom, total_training)\nprint(model2)\nsummary(model2)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}